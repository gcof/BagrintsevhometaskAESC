{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Dvm6MmIGddQw"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"jgcJ2LJ2ddQz"},"source":["## 1. Q-learning in the wild (3 pts)\n","\n","Here we use the qlearning agent on taxi env from openai gym.\n","You will need to insert a few agent functions here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6uJVRBJddQ1"},"outputs":[],"source":["import random,math\n","import numpy as np\n","from collections import defaultdict\n","\n","class QLearningAgent():\n","  \"\"\"\n","    Q-Learning Agent\n","\n","    Instance variables you have access to\n","      - self.epsilon (exploration prob)\n","      - self.alpha (learning rate)\n","      - self.discount (discount rate aka gamma)\n","\n","    Functions you should use\n","      - self.getLegalActions(state)\n","        which returns legal actions for a state\n","      - self.getQValue(state,action)\n","        which returns Q(state,action)\n","      - self.setQValue(state,action,value)\n","        which sets Q(state,action) := value\n","\n","    !!!Important!!!\n","    NOTE: please avoid using self._qValues directly to make code cleaner\n","  \"\"\"\n","  def __init__(self,alpha,epsilon,discount,getLegalActions):\n","    \"We initialize agent and Q-values here.\"\n","    self.getLegalActions= getLegalActions\n","    self._qValues = defaultdict(lambda:defaultdict(lambda:0))\n","    self.alpha = alpha\n","    self.epsilon = epsilon\n","    self.discount = discount\n","\n","  def getQValue(self, state, action):\n","    #print(state)\n","    #print(action)\n","    if not (state in self._qValues) or not (action in self._qValues[state]):\n","        return 0.0\n","    return self._qValues[state][action]\n","\n","  def setQValue(self,state,action,value):\n","    \"\"\"\n","      Sets the Qvalue for [state,action] to the given value\n","    \"\"\"\n","    self._qValues[state][action] = value\n","\n","#---------------------#start of your code#---------------------#\n","\n","  def getValue(self, state):\n","    \"\"\"\n","      Returns max_action Q(state,action)\n","      where the max is over legal actions.\n","    \"\"\"\n","\n","    possibleActions =\n","    #If there are no legal actions, return 0.0\n","\n","    return #\n","\n","  def getPolicy(self, state):\n","    \"\"\"\n","      Compute the best action to take in a state.\n","\n","    \"\"\"\n","    possibleActions = self.getLegalActions(state)\n","\n","    #If there are no legal actions, return None\n","\n","    return #\n","\n","  def getAction(self, state):\n","    \"\"\"\n","      Compute the action to take in the current state, including exploration.\n","\n","      With probability self.epsilon, we should take a random action.\n","      otherwise - the best policy action (self.getPolicy).\n","\n","      HINT: You might want to use util.flipCoin(prob)\n","      HINT: To pick randomly from a list, use random.choice(list)\n","\n","    \"\"\"\n","\n","    # Pick Action\n","\n","    #If there are no legal actions, return None\n","\n","    #choose action with epsilon exploration strategy:\n","\n","    return action\n","\n","  def update(self, state, action, nextState, reward):\n","    \"\"\"\n","      You should do your Q-Value update here\n","\n","      NOTE: You should never call this function,\n","      it will be called on your behalf\n","\n","\n","    \"\"\"\n","    #agent parameters\n","    gamma = self.discount\n","    learning_rate = self.alpha\n","\n","    \"*** YOUR CODE HERE ***\"\n","    self.setQValue(#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IyXE8SzddQ2"},"outputs":[],"source":["import gym\n","env = gym.make(\"Taxi-v3\")\n","n_actions = env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TFzQrJJddQ2"},"outputs":[],"source":["def play_and_train(env,agent,t_max=10**4):\n","    \"\"\"This function should\n","    - run a full game, actions given by agent.getAction(s)\n","    - train agent using agent.update(...) whenever possible\n","    - return total reward\"\"\"\n","    total_reward = 0.0\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        a = agent.getAction(s)\n","\n","        next_s,r,done,_ = env.step(a)\n","\n","        #<train(update) agent for state s>\n","\n","        s = next_s\n","        total_reward +=r\n","        if done:break\n","\n","    return total_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhsvXf9BddQ2"},"outputs":[],"source":["agent = QLearningAgent(alpha=,epsilon=,discount=,\n","                       getLegalActions = lambda s: range(n_actions))"]},{"cell_type":"markdown","metadata":{"id":"_ZwJKCfvddQ3"},"source":["Достигните положительной награды, постройте график"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zo6QbdGxddQ3"},"outputs":[],"source":["from IPython.display import clear_output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48CrrrxGddQ3"},"outputs":[],"source":["from IPython.display import clear_output\n","rewards = []\n","for i in range(1000):\n","    rewards.append(play_and_train(env,agent))\n","    agent.epsilon # уменьшайте реплей со временем\n","    if i % 100 ==0:\n","        clear_output(True)\n","        print(agent.epsilon)\n","        plt.plot(rewards)\n","        plt.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"gxQu70I_ddQ4"},"source":["## 3. Continuous state space (2 pt)\n","\n","Чтобы использовать табличный q-learning на continuous состояниях, надо как-то их обрабатывать и бинаризовать. Придумайте способ разбивки на дискретные состояния."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4eD9gTjddQ4"},"outputs":[],"source":["env = gym.make(\"CartPole-v0\")\n","n_actions = env.action_space.n\n","print(\"first state:%s\"%(env.reset()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6V68sMQddQ4"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"TUM1t0jvddQ4"},"source":["### Play a few games\n","\n","Постройте распределения различных частей состояния игры. Сыграйте несколько игр и запишите все состояния."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnHeBx5XddQ5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3zZHwCUfddQ5"},"source":["## Binarize environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6f-DyizddQ5"},"outputs":[],"source":["from gym.core import ObservationWrapper\n","class Binarizer(ObservationWrapper):\n","\n","    def to_bin(self, value, bins):\n","\n","        return\n","\n","    def _observation(self,state):\n","\n","        state = (self.to_bin(state[0], ), self.to_bin(state[1], ), self.to_bin(state[2], ), self.to_bin(state[3], ))\n","\n","        return state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpaDIw43ddQ5"},"outputs":[],"source":["env = Binarizer(gym.make(\"CartPole-v0\"))"]},{"cell_type":"markdown","metadata":{"id":"LkZCEBD_ddQ5"},"source":["## Learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sFMtxZYddQ5"},"outputs":[],"source":["agent = QLearningAgent(alpha=,epsilon=,discount=,\n","                       getLegalActions = lambda s: range(n_actions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2uuuA5bddQ5"},"outputs":[],"source":["rewards = []\n","rewBuf = []\n","ma = -1000000000000\n","for i in range(10000):\n","    for i in range(100):\n","        rewards.append(play_and_train(env,agent))\n","    agent.epsilon *= #\n","    rewBuf.append(np.mean(rewards[-100:]))\n","    clear_output(True)\n","    print(agent.epsilon)\n","    print(rewBuf[-1])\n","    plt.plot(rewBuf)\n","    if(rewBuf[-1] > 195):\n","        print(\"Win!\")\n","        break\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"0GCEU6g1ddQ5"},"source":["## 4. Experience replay (5 pts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQ_3uojDddQ5"},"outputs":[],"source":["import random\n","class ReplayBuffer(object):\n","    def __init__(self, size):\n","        \"\"\"Create Replay buffer.\n","        Parameters\n","        ----------\n","        size: int\n","            Max number of transitions to store in the buffer. When the buffer\n","            overflows the old memories are dropped.\n","        \"\"\"\n","        self._storage = []\n","        self._maxsize = size\n","        self._replaceId = 0\n","\n","\n","    def __len__(self):\n","        return len(self._storage)\n","\n","    def add(self, obs_t, action, reward, obs_tp1, done):\n","        '''\n","        Make sure, _storage will not exceed _maxsize.\n","        '''\n","        data = (obs_t, action, reward, obs_tp1, done)\n","        if len(self._storage) == self._maxsize:\n","            #\n","        else:\n","            #\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch of experiences.\n","        Parameters\n","        ----------\n","        batch_size: int\n","            How many transitions to sample.\n","        Returns\n","        -------\n","        obs_batch: np.array\n","            batch of observations\n","        act_batch: np.array\n","            batch of actions executed given obs_batch\n","        rew_batch: np.array\n","            rewards received as results of executing act_batch\n","        next_obs_batch: np.array\n","            next set of observations seen after executing act_batch\n","        done_mask: np.array\n","            done_mask[i] = 1 if executing act_batch[i] resulted in\n","            the end of an episode and 0 otherwise.\n","        \"\"\"\n","\n","        #\n","\n","        return states, actions, rewards, next_states, is_done\n"]},{"cell_type":"markdown","metadata":{"id":"7KkoMo-EddQ6"},"source":["Some tests to make sure your buffer works right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ze99MZrmddQ6"},"outputs":[],"source":["import numpy as np\n","replay = ReplayBuffer(2)\n","obj1 = tuple(range(5))\n","obj2 = tuple(range(5, 10))\n","replay.add(*obj1)\n","assert replay.sample(1)==obj1, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n","replay.add(*obj2)\n","assert len(replay._storage)==2, \"Please make sure __len__ methods works as intended.\"\n","replay.add(*obj2)\n","assert len(replay._storage)==2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n","assert tuple(np.unique(a) for a in replay.sample(100))==obj2\n","replay.add(*obj1)\n","assert max(len(np.unique(a)) for a in replay.sample(100))==2\n","replay.add(*obj1)\n","assert tuple(np.unique(a) for a in replay.sample(100))==obj1\n","print (\"Success!\")"]},{"cell_type":"markdown","metadata":{"id":"RaImoEEgddQ6"},"source":["Now let's use this buffer to improve training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czwcYYYmddQ6"},"outputs":[],"source":["import gym\n","env = Binarizer(gym.make('CartPole-v0'))\n","n_actions = env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChtwaobcddQ6"},"outputs":[],"source":["agent = QLearningAgent(alpha=,epsilon=,discount=,\n","                       getLegalActions = lambda s: range(n_actions))\n","replay = ReplayBuffer(10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdKXIULcddQ6"},"outputs":[],"source":["def play_and_train(env, agent, t_max=10**4, batch_size=10):\n","    \"\"\"This function should\n","    - run a full game, actions given by agent.getAction(s)\n","    - train agent using agent.update(...) whenever possible\n","    - return total reward\"\"\"\n","    total_reward = 0.0\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        aсtion = agent.getAction(s)\n","        next_s, r, done,_ = env.step(aсtion)\n","\n","        #заполните реплей\n","        #опционально - моежте также как в варианте без реплея обучаться по состояниям которые\n","\n","        s = next_s\n","        total_reward +=r\n","        if done:break\n","\n","    #learn from replay\n","\n","    return total_reward\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"rcB2f2GaddQ6"},"source":["Train with experience replay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Lqmk0S_ddQ6"},"outputs":[],"source":["rewards = []\n","rewBuf = []\n","ma = -1000000000000\n","for i in range(10000):\n","    for i in range(100):\n","        rewards.append(play_and_train(env,agent, batch_size=1000))\n","    agent.epsilon *= #\n","    rewBuf.append(np.mean(rewards[-100:]))\n","    clear_output(True)\n","    print(agent.epsilon)\n","    print(rewBuf[-1])\n","    plt.plot(rewBuf)\n","    if(rewBuf[-1] > 195):\n","        print(\"Win!\")\n","        break\n","    plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}